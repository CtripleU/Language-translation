{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "metadata": {
        "id": "FH6Nv9JLO5XQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "\n",
        "dataset = load_dataset(\"ccibeekeoc42/english_to_igbo\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u5WnyarpBZJX",
        "outputId": "0e6d0cdf-4232-4432-f496-88bc255de02a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# show dataset details\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPYhgGRZCRV9",
        "outputId": "1d3a6882-dd8b-4af7-8ee4-27ad28beaf6f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['English', 'Igbo'],\n",
              "        num_rows: 522322\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['English', 'Igbo'],\n",
              "        num_rows: 3296\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert training set to a pandas dataframe\n",
        "train_df = dataset['train'].to_pandas()\n",
        "\n",
        "# convert test set to a pandas dataframe\n",
        "test_df = dataset['test'].to_pandas()"
      ],
      "metadata": {
        "id": "yR2FJNfsJf5Z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display the first few rows of the training set\n",
        "print(train_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcBTZV9fK198",
        "outputId": "4bce736b-af3b-41ca-ebce-25cbecf95a67"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             English  \\\n",
            "0  All this was happening amidst a barrage of reg...   \n",
            "1  Soon after Jota was denied by the recovering D...   \n",
            "2  Friday’s rout equalled Manchester United’s 9-0...   \n",
            "3  There were over 70 million Nigerians on the hi...   \n",
            "4  Diet and Dementia: How you will cure dementia ...   \n",
            "\n",
            "                                                Igbo  \n",
            "0  Ihe a niile na-eme n' etiti ọtụtụ twiit nke si...  \n",
            "1  Ozigbo David Sanchez na-agbake agbake gọnahara...  \n",
            "2  Asọmụmpi nke Fụraịde nke Manchester United ji ...  \n",
            "3  E nwere ihe karịrị nde ndị Naịjirịa iri asaa n...  \n",
            "4  Diet and Dementia: Etu ị ga-esi jiri chocolate...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display the first few rows of the test set\n",
        "print(test_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRhhb_MJK5a4",
        "outputId": "94ccf560-a75c-4280-de44-859e69a97b6c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             English  \\\n",
            "0  The latest report reaching us is that it's rem...   \n",
            "1       Why did you leave your former place of work?   \n",
            "2  Majozi is a politics and international affairs...   \n",
            "3                Saraki: The police plan has changed   \n",
            "4  'Ekechi said that they had about 40 videos whi...   \n",
            "\n",
            "                                                Igbo  \n",
            "0  Nke ọhụrụ na-eru anyị ntị ugbua na-ekwu na ọ o...  \n",
            "1        Gịnị mere i ji hapụ ebe ị na-arụ n'oge mbu?  \n",
            "2  Majozi bụ onye nyocha ndọrọ ndọrọ ọchịchị na o...  \n",
            "3               Saraki: egwu ndị uweojii adagharịala  \n",
            "4  Ekechi kwuru na ha nwere ihe onyonyo ruru 40 g...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "bjupBSnTLdRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check for missing values\n",
        "\n",
        "print(train_df.isnull().sum())\n",
        "print(test_df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4IUuRpRhMWJ",
        "outputId": "3b550183-7f19-4c4f-e73a-e32469872dc5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English    0\n",
            "Igbo       0\n",
            "dtype: int64\n",
            "English    0\n",
            "Igbo       0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # lowercasing\n",
        "    text = text.lower()\n",
        "    # removing special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "pK6MDqMTP0Jx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['English'] = train_df['English'].apply(preprocess_text)\n",
        "train_df['Igbo'] = train_df['Igbo'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "7jgO4q_pZZ7x"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['English'] = test_df['English'].apply(preprocess_text)\n",
        "test_df['Igbo'] = test_df['Igbo'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "NpcHLl3iZqxB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the training set into training and validation set\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "DRO6e3A7Oo-Q"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize and convert to sequences\n",
        "tokenizer_eng = Tokenizer()\n",
        "tokenizer_igbo = Tokenizer()\n",
        "\n",
        "tokenizer_eng.fit_on_texts(train_df['English'])\n",
        "tokenizer_igbo.fit_on_texts(train_df['Igbo'])\n",
        "\n",
        "train_sequences_eng = tokenizer_eng.texts_to_sequences(train_df['English'])\n",
        "train_sequences_igbo = tokenizer_igbo.texts_to_sequences(train_df['Igbo'])\n",
        "\n",
        "val_sequences_eng = tokenizer_eng.texts_to_sequences(val_df['English'])\n",
        "val_sequences_igbo = tokenizer_igbo.texts_to_sequences(val_df['Igbo'])\n",
        "\n",
        "test_sequences_eng = tokenizer_eng.texts_to_sequences(test_df['English'])\n",
        "test_sequences_igbo = tokenizer_igbo.texts_to_sequences(test_df['Igbo'])"
      ],
      "metadata": {
        "id": "KB8BZiXPdbpm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pad sequences to the same length\n",
        "max_len_eng = max(max(len(seq) for seq in train_sequences_eng), max(len(seq) for seq in val_sequences_eng), max(len(seq) for seq in test_sequences_eng))\n",
        "max_len_igbo = max(max(len(seq) for seq in train_sequences_igbo), max(len(seq) for seq in val_sequences_igbo), max(len(seq) for seq in test_sequences_igbo))\n",
        "\n",
        "train_padded_eng = pad_sequences(train_sequences_eng, maxlen=max_len_eng, padding='post')\n",
        "train_padded_igbo = pad_sequences(train_sequences_igbo, maxlen=max_len_igbo, padding='post')\n",
        "\n",
        "val_padded_eng = pad_sequences(val_sequences_eng, maxlen=max_len_eng, padding='post')\n",
        "val_padded_igbo = pad_sequences(val_sequences_igbo, maxlen=max_len_igbo, padding='post')\n",
        "\n",
        "test_padded_eng = pad_sequences(test_sequences_eng, maxlen=max_len_eng, padding='post')\n",
        "test_padded_igbo = pad_sequences(test_sequences_igbo, maxlen=max_len_igbo, padding='post')"
      ],
      "metadata": {
        "id": "06xiBrDkdvEt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define vocabulary sizes\n",
        "vocab_size_eng = len(tokenizer_eng.word_index) + 1\n",
        "vocab_size_igbo = len(tokenizer_igbo.word_index) + 1\n",
        "\n",
        "print(f'Vocabulary size (English): {vocab_size_eng}')\n",
        "print(f'Vocabulary size (Igbo): {vocab_size_igbo}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_7gS6gXeByo",
        "outputId": "812027b9-a387-461f-8f61-b1bd338f9e6e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size (English): 63474\n",
            "Vocabulary size (Igbo): 78827\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define model parameters\n",
        "embedding_dim = 256\n",
        "latent_dim = 512"
      ],
      "metadata": {
        "id": "FbjzvGiJeL7N"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model building"
      ],
      "metadata": {
        "id": "DBIRbOKefUos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder\n",
        "encoder_inputs = Input(shape=(max_len_eng,), name='encoder_inputs')\n",
        "encoder_embedding = Embedding(input_dim=vocab_size_eng, output_dim=embedding_dim, mask_zero=True, name='encoder_embedding')(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True, name='encoder_lstm')\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# decoder\n",
        "decoder_inputs = Input(shape=(max_len_igbo,), name='decoder_inputs')\n",
        "decoder_embedding = Embedding(input_dim=vocab_size_igbo, output_dim=embedding_dim, mask_zero=True, name='decoder_embedding')(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(vocab_size_igbo, activation='softmax', name='decoder_dense')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n"
      ],
      "metadata": {
        "id": "XTu8joFXePvy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "l3Ud1X4ufyxM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "marxlKB-ffr2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qv9QzlqWfjDE",
        "outputId": "6115c5cd-9474-4c84-b061-25fbe9161881"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer  [(None, 1132)]               0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " decoder_inputs (InputLayer  [(None, 2071)]               0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " encoder_embedding (Embeddi  (None, 1132, 256)            1624934   ['encoder_inputs[0][0]']      \n",
            " ng)                                                      4                                       \n",
            "                                                                                                  \n",
            " decoder_embedding (Embeddi  (None, 2071, 256)            2017971   ['decoder_inputs[0][0]']      \n",
            " ng)                                                      2                                       \n",
            "                                                                                                  \n",
            " encoder_lstm (LSTM)         [(None, 512),                1574912   ['encoder_embedding[0][0]']   \n",
            "                              (None, 512),                                                        \n",
            "                              (None, 512)]                                                        \n",
            "                                                                                                  \n",
            " decoder_lstm (LSTM)         [(None, 2071, 512),          1574912   ['decoder_embedding[0][0]',   \n",
            "                              (None, 512),                           'encoder_lstm[0][1]',        \n",
            "                              (None, 512)]                           'encoder_lstm[0][2]']        \n",
            "                                                                                                  \n",
            " decoder_dense (Dense)       (None, 2071, 78827)          4043825   ['decoder_lstm[0][0]']        \n",
            "                                                          1                                       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 80017131 (305.24 MB)\n",
            "Trainable params: 80017131 (305.24 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "SbYZRPbBgBDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prep target data\n",
        "train_target_igbo = train_padded_igbo[:, 1:]\n",
        "val_target_igbo = val_padded_igbo[:, 1:]\n",
        "test_target_igbo = test_padded_igbo[:, 1:]\n",
        "\n",
        "# train model\n",
        "history = model.fit(\n",
        "    [train_padded_eng, train_padded_igbo[:, :-1]], train_target_igbo,\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    validation_data=([val_padded_eng, val_padded_igbo[:, :-1]], val_target_igbo)\n",
        ")"
      ],
      "metadata": {
        "id": "_xctwhX4f2Zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model evaluation"
      ],
      "metadata": {
        "id": "rmvgLC6UgJcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(\n",
        "    [test_padded_eng, test_padded_igbo[:, :-1]],\n",
        "    test_target_igbo\n",
        ")\n",
        "\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "id": "-Te5-8kfgL8s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}