{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (3.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (0.23.3)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FH6Nv9JLO5XQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 07:16:44.640119: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-14 07:16:44.640172: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-14 07:16:44.799953: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-14 07:16:45.123816: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-14 07:16:47.120281: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u5WnyarpBZJX",
    "outputId": "0e6d0cdf-4232-4432-f496-88bc255de02a"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "dataset = load_dataset(\"ccibeekeoc42/english_to_igbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UPYhgGRZCRV9",
    "outputId": "1d3a6882-dd8b-4af7-8ee4-27ad28beaf6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['English', 'Igbo'],\n",
       "        num_rows: 522322\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['English', 'Igbo'],\n",
       "        num_rows: 3296\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show dataset details\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yR2FJNfsJf5Z"
   },
   "outputs": [],
   "source": [
    "# convert training set to a pandas dataframe\n",
    "train_df = dataset['train'].to_pandas()\n",
    "\n",
    "# convert test set to a pandas dataframe\n",
    "test_df = dataset['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KcBTZV9fK198",
    "outputId": "4bce736b-af3b-41ca-ebce-25cbecf95a67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             English  \\\n",
      "0  All this was happening amidst a barrage of reg...   \n",
      "1  Soon after Jota was denied by the recovering D...   \n",
      "2  Friday’s rout equalled Manchester United’s 9-0...   \n",
      "3  There were over 70 million Nigerians on the hi...   \n",
      "4  Diet and Dementia: How you will cure dementia ...   \n",
      "\n",
      "                                                Igbo  \n",
      "0  Ihe a niile na-eme n' etiti ọtụtụ twiit nke si...  \n",
      "1  Ozigbo David Sanchez na-agbake agbake gọnahara...  \n",
      "2  Asọmụmpi nke Fụraịde nke Manchester United ji ...  \n",
      "3  E nwere ihe karịrị nde ndị Naịjirịa iri asaa n...  \n",
      "4  Diet and Dementia: Etu ị ga-esi jiri chocolate...  \n"
     ]
    }
   ],
   "source": [
    "# display the first few rows of the training set\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QRhhb_MJK5a4",
    "outputId": "94ccf560-a75c-4280-de44-859e69a97b6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             English  \\\n",
      "0  The latest report reaching us is that it's rem...   \n",
      "1       Why did you leave your former place of work?   \n",
      "2  Majozi is a politics and international affairs...   \n",
      "3                Saraki: The police plan has changed   \n",
      "4  'Ekechi said that they had about 40 videos whi...   \n",
      "\n",
      "                                                Igbo  \n",
      "0  Nke ọhụrụ na-eru anyị ntị ugbua na-ekwu na ọ o...  \n",
      "1        Gịnị mere i ji hapụ ebe ị na-arụ n'oge mbu?  \n",
      "2  Majozi bụ onye nyocha ndọrọ ndọrọ ọchịchị na o...  \n",
      "3               Saraki: egwu ndị uweojii adagharịala  \n",
      "4  Ekechi kwuru na ha nwere ihe onyonyo ruru 40 g...  \n"
     ]
    }
   ],
   "source": [
    "# display the first few rows of the test set\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjupBSnTLdRG"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U4IUuRpRhMWJ",
    "outputId": "3b550183-7f19-4c4f-e73a-e32469872dc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English    0\n",
      "Igbo       0\n",
      "dtype: int64\n",
      "English    0\n",
      "Igbo       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check for missing values\n",
    "\n",
    "print(train_df.isnull().sum())\n",
    "print(test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pK6MDqMTP0Jx"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # lowercasing\n",
    "    text = text.lower()\n",
    "    # removing special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7jgO4q_pZZ7x"
   },
   "outputs": [],
   "source": [
    "train_df['English'] = train_df['English'].apply(preprocess_text)\n",
    "train_df['Igbo'] = train_df['Igbo'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NpcHLl3iZqxB"
   },
   "outputs": [],
   "source": [
    "test_df['English'] = test_df['English'].apply(preprocess_text)\n",
    "test_df['Igbo'] = test_df['Igbo'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "DRO6e3A7Oo-Q"
   },
   "outputs": [],
   "source": [
    "# split the training set into training and validation set\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KB8BZiXPdbpm"
   },
   "outputs": [],
   "source": [
    "# tokenize and convert to sequences\n",
    "tokenizer_eng = Tokenizer()\n",
    "tokenizer_igbo = Tokenizer()\n",
    "\n",
    "tokenizer_eng.fit_on_texts(train_df['English'])\n",
    "tokenizer_igbo.fit_on_texts(train_df['Igbo'])\n",
    "\n",
    "train_sequences_eng = tokenizer_eng.texts_to_sequences(train_df['English'])\n",
    "train_sequences_igbo = tokenizer_igbo.texts_to_sequences(train_df['Igbo'])\n",
    "\n",
    "val_sequences_eng = tokenizer_eng.texts_to_sequences(val_df['English'])\n",
    "val_sequences_igbo = tokenizer_igbo.texts_to_sequences(val_df['Igbo'])\n",
    "\n",
    "test_sequences_eng = tokenizer_eng.texts_to_sequences(test_df['English'])\n",
    "test_sequences_igbo = tokenizer_igbo.texts_to_sequences(test_df['Igbo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "06xiBrDkdvEt"
   },
   "outputs": [],
   "source": [
    "# pad sequences to the same length\n",
    "max_len_eng = max(max(len(seq) for seq in train_sequences_eng), max(len(seq) for seq in val_sequences_eng), max(len(seq) for seq in test_sequences_eng))\n",
    "max_len_igbo = max(max(len(seq) for seq in train_sequences_igbo), max(len(seq) for seq in val_sequences_igbo), max(len(seq) for seq in test_sequences_igbo))\n",
    "\n",
    "train_padded_eng = pad_sequences(train_sequences_eng, maxlen=max_len_eng, padding='post')\n",
    "train_padded_igbo = pad_sequences(train_sequences_igbo, maxlen=max_len_igbo, padding='post')\n",
    "\n",
    "val_padded_eng = pad_sequences(val_sequences_eng, maxlen=max_len_eng, padding='post')\n",
    "val_padded_igbo = pad_sequences(val_sequences_igbo, maxlen=max_len_igbo, padding='post')\n",
    "\n",
    "test_padded_eng = pad_sequences(test_sequences_eng, maxlen=max_len_eng, padding='post')\n",
    "test_padded_igbo = pad_sequences(test_sequences_igbo, maxlen=max_len_igbo, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_7gS6gXeByo",
    "outputId": "812027b9-a387-461f-8f61-b1bd338f9e6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size (English): 63474\n",
      "Vocabulary size (Igbo): 78827\n"
     ]
    }
   ],
   "source": [
    "# define vocabulary sizes\n",
    "vocab_size_eng = len(tokenizer_eng.word_index) + 1\n",
    "vocab_size_igbo = len(tokenizer_igbo.word_index) + 1\n",
    "\n",
    "print(f'Vocabulary size (English): {vocab_size_eng}')\n",
    "print(f'Vocabulary size (Igbo): {vocab_size_igbo}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "FbjzvGiJeL7N"
   },
   "outputs": [],
   "source": [
    "# define model parameters\n",
    "embedding_dim = 256\n",
    "latent_dim = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBIRbOKefUos"
   },
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "XTu8joFXePvy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 07:17:34.015743: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "# encoder\n",
    "encoder_inputs = Input(shape=(max_len_eng,), name='encoder_inputs')\n",
    "encoder_embedding = Embedding(input_dim=vocab_size_eng, output_dim=embedding_dim, mask_zero=True, name='encoder_embedding')(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True, name='encoder_lstm')\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# decoder\n",
    "decoder_inputs = Input(shape=(max_len_igbo,), name='decoder_inputs')\n",
    "decoder_embedding = Embedding(input_dim=vocab_size_igbo, output_dim=embedding_dim, mask_zero=True, name='decoder_embedding')(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size_igbo, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "l3Ud1X4ufyxM"
   },
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "marxlKB-ffr2"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qv9QzlqWfjDE",
    "outputId": "6115c5cd-9474-4c84-b061-25fbe9161881"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer  [(None, 1132)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer  [(None, 2071)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " encoder_embedding (Embeddi  (None, 1132, 256)            1624934   ['encoder_inputs[0][0]']      \n",
      " ng)                                                      4                                       \n",
      "                                                                                                  \n",
      " decoder_embedding (Embeddi  (None, 2071, 256)            2017971   ['decoder_inputs[0][0]']      \n",
      " ng)                                                      2                                       \n",
      "                                                                                                  \n",
      " encoder_lstm (LSTM)         [(None, 512),                1574912   ['encoder_embedding[0][0]']   \n",
      "                              (None, 512),                                                        \n",
      "                              (None, 512)]                                                        \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)         [(None, 2071, 512),          1574912   ['decoder_embedding[0][0]',   \n",
      "                              (None, 512),                           'encoder_lstm[0][1]',        \n",
      "                              (None, 512)]                           'encoder_lstm[0][2]']        \n",
      "                                                                                                  \n",
      " decoder_dense (Dense)       (None, 2071, 78827)          4043825   ['decoder_lstm[0][0]']        \n",
      "                                                          1                                       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 80017131 (305.24 MB)\n",
      "Trainable params: 80017131 (305.24 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbYZRPbBgBDU"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prep target data\n",
    "train_target_igbo = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    train_padded_igbo[:, 1:], maxlen=max_len_igbo, padding='pre', value=0.0\n",
    ")\n",
    "val_target_igbo = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    val_padded_igbo[:, 1:], maxlen=max_len_igbo, padding='pre', value=0.0\n",
    ")\n",
    "test_target_igbo = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    test_padded_igbo[:, 1:], maxlen=max_len_igbo, padding='pre', value=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 07:18:29.193383: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 10448045888 exceeds 10% of free system memory.\n",
      "2024-06-14 07:18:32.665674: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 10448045888 exceeds 10% of free system memory.\n",
      "2024-06-14 07:18:32.665741: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 10448045888 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(\n",
    "    [train_padded_eng, train_padded_igbo], train_target_igbo,\n",
    "    epochs=20,\n",
    "    batch_size=16,\n",
    "    validation_data=([val_padded_eng, val_padded_igbo], val_target_igbo)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmvgLC6UgJcN"
   },
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Te5-8kfgL8s"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(\n",
    "    [test_padded_eng, test_padded_igbo[:, :-1]],\n",
    "    test_target_igbo\n",
    ")\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
