{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (3.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (0.23.3)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FH6Nv9JLO5XQ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 09:10:00.344128: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-14 09:10:00.376369: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-14 09:10:00.376390: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-14 09:10:00.377227: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-14 09:10:00.382471: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-14 09:10:01.003439: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u5WnyarpBZJX",
    "outputId": "0e6d0cdf-4232-4432-f496-88bc255de02a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "dataset = load_dataset(\"ccibeekeoc42/english_to_igbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UPYhgGRZCRV9",
    "outputId": "1d3a6882-dd8b-4af7-8ee4-27ad28beaf6f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['English', 'Igbo'],\n",
       "        num_rows: 522322\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['English', 'Igbo'],\n",
       "        num_rows: 3296\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show dataset details\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yR2FJNfsJf5Z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert training set to a pandas dataframe\n",
    "train_df = dataset['train'].to_pandas()\n",
    "\n",
    "# convert test set to a pandas dataframe\n",
    "test_df = dataset['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample 10000 rows from the training dataset\n",
    "train_df = train_df.sample(n=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KcBTZV9fK198",
    "outputId": "4bce736b-af3b-41ca-ebce-25cbecf95a67",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  English  \\\n",
      "34150                   gallese - Sepa (Papua New Guinea)   \n",
      "76521                                             Awake !   \n",
      "190885  ‘ Eyesalve to Rub in Eyes , ’ 12 / 15 Golden R...   \n",
      "108526                                    [ Credit Line ]   \n",
      "278129                                            8 : 8 .   \n",
      "\n",
      "                                                     Igbo  \n",
      "34150                     Ghari - Sepa (Papua New Guinea)  \n",
      "76521                                              Teta !  \n",
      "190885  ‘ Ọgwụ Anya Ite n’Anya Gị , ’ 12 / 15 Paradaịs...  \n",
      "108526                            [ Ebe E Si Nweta Foto ]  \n",
      "278129                                            8 : 8 .  \n"
     ]
    }
   ],
   "source": [
    "# display the first few rows of the training set\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QRhhb_MJK5a4",
    "outputId": "94ccf560-a75c-4280-de44-859e69a97b6c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             English  \\\n",
      "0  The latest report reaching us is that it's rem...   \n",
      "1       Why did you leave your former place of work?   \n",
      "2  Majozi is a politics and international affairs...   \n",
      "3                Saraki: The police plan has changed   \n",
      "4  'Ekechi said that they had about 40 videos whi...   \n",
      "\n",
      "                                                Igbo  \n",
      "0  Nke ọhụrụ na-eru anyị ntị ugbua na-ekwu na ọ o...  \n",
      "1        Gịnị mere i ji hapụ ebe ị na-arụ n'oge mbu?  \n",
      "2  Majozi bụ onye nyocha ndọrọ ndọrọ ọchịchị na o...  \n",
      "3               Saraki: egwu ndị uweojii adagharịala  \n",
      "4  Ekechi kwuru na ha nwere ihe onyonyo ruru 40 g...  \n"
     ]
    }
   ],
   "source": [
    "# display the first few rows of the test set\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjupBSnTLdRG"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U4IUuRpRhMWJ",
    "outputId": "3b550183-7f19-4c4f-e73a-e32469872dc5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English    0\n",
      "Igbo       0\n",
      "dtype: int64\n",
      "English    0\n",
      "Igbo       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check for missing values\n",
    "\n",
    "print(train_df.isnull().sum())\n",
    "print(test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pK6MDqMTP0Jx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # lowercasing\n",
    "    text = text.lower()\n",
    "    # removing special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7jgO4q_pZZ7x",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['English'] = train_df['English'].apply(preprocess_text)\n",
    "train_df['Igbo'] = train_df['Igbo'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NpcHLl3iZqxB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df['English'] = test_df['English'].apply(preprocess_text)\n",
    "test_df['Igbo'] = test_df['Igbo'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DRO6e3A7Oo-Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split the training set into training and validation set\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "KB8BZiXPdbpm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenize and convert to sequences\n",
    "tokenizer_eng = Tokenizer()\n",
    "tokenizer_igbo = Tokenizer()\n",
    "\n",
    "tokenizer_eng.fit_on_texts(train_df['English'])\n",
    "tokenizer_igbo.fit_on_texts(train_df['Igbo'])\n",
    "\n",
    "train_sequences_eng = tokenizer_eng.texts_to_sequences(train_df['English'])\n",
    "train_sequences_igbo = tokenizer_igbo.texts_to_sequences(train_df['Igbo'])\n",
    "\n",
    "val_sequences_eng = tokenizer_eng.texts_to_sequences(val_df['English'])\n",
    "val_sequences_igbo = tokenizer_igbo.texts_to_sequences(val_df['Igbo'])\n",
    "\n",
    "test_sequences_eng = tokenizer_eng.texts_to_sequences(test_df['English'])\n",
    "test_sequences_igbo = tokenizer_igbo.texts_to_sequences(test_df['Igbo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "06xiBrDkdvEt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pad sequences to the same length\n",
    "max_len_eng = max(max(len(seq) for seq in train_sequences_eng), max(len(seq) for seq in val_sequences_eng), max(len(seq) for seq in test_sequences_eng))\n",
    "max_len_igbo = max(max(len(seq) for seq in train_sequences_igbo), max(len(seq) for seq in val_sequences_igbo), max(len(seq) for seq in test_sequences_igbo))\n",
    "\n",
    "train_padded_eng = pad_sequences(train_sequences_eng, maxlen=max_len_eng, padding='post')\n",
    "train_padded_igbo = pad_sequences(train_sequences_igbo, maxlen=max_len_igbo, padding='post')\n",
    "\n",
    "val_padded_eng = pad_sequences(val_sequences_eng, maxlen=max_len_eng, padding='post')\n",
    "val_padded_igbo = pad_sequences(val_sequences_igbo, maxlen=max_len_igbo, padding='post')\n",
    "\n",
    "test_padded_eng = pad_sequences(test_sequences_eng, maxlen=max_len_eng, padding='post')\n",
    "test_padded_igbo = pad_sequences(test_sequences_igbo, maxlen=max_len_igbo, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_7gS6gXeByo",
    "outputId": "812027b9-a387-461f-8f61-b1bd338f9e6e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size (English): 11784\n",
      "Vocabulary size (Igbo): 9629\n"
     ]
    }
   ],
   "source": [
    "# define vocabulary sizes\n",
    "vocab_size_eng = len(tokenizer_eng.word_index) + 1\n",
    "vocab_size_igbo = len(tokenizer_igbo.word_index) + 1\n",
    "\n",
    "print(f'Vocabulary size (English): {vocab_size_eng}')\n",
    "print(f'Vocabulary size (Igbo): {vocab_size_igbo}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "FbjzvGiJeL7N",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define model parameters\n",
    "embedding_dim = 256\n",
    "latent_dim = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBIRbOKefUos"
   },
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "XTu8joFXePvy",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 09:10:03.312082: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "# encoder\n",
    "encoder_inputs = Input(shape=(max_len_eng,), name='encoder_inputs')\n",
    "encoder_embedding = Embedding(input_dim=vocab_size_eng, output_dim=embedding_dim, mask_zero=True, name='encoder_embedding')(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True, name='encoder_lstm')\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# decoder\n",
    "decoder_inputs = Input(shape=(max_len_igbo,), name='decoder_inputs')\n",
    "decoder_embedding = Embedding(input_dim=vocab_size_igbo, output_dim=embedding_dim, mask_zero=True, name='decoder_embedding')(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size_igbo, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "l3Ud1X4ufyxM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "marxlKB-ffr2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qv9QzlqWfjDE",
    "outputId": "6115c5cd-9474-4c84-b061-25fbe9161881",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer  [(None, 973)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer  [(None, 140)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " encoder_embedding (Embeddi  (None, 973, 256)             3016704   ['encoder_inputs[0][0]']      \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " decoder_embedding (Embeddi  (None, 140, 256)             2465024   ['decoder_inputs[0][0]']      \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " encoder_lstm (LSTM)         [(None, 512),                1574912   ['encoder_embedding[0][0]']   \n",
      "                              (None, 512),                                                        \n",
      "                              (None, 512)]                                                        \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)         [(None, 140, 512),           1574912   ['decoder_embedding[0][0]',   \n",
      "                              (None, 512),                           'encoder_lstm[0][1]',        \n",
      "                              (None, 512)]                           'encoder_lstm[0][2]']        \n",
      "                                                                                                  \n",
      " decoder_dense (Dense)       (None, 140, 9629)            4939677   ['decoder_lstm[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13571229 (51.77 MB)\n",
      "Trainable params: 13571229 (51.77 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbYZRPbBgBDU"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prep target data\n",
    "train_target_igbo = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    train_padded_igbo[:, 1:], maxlen=max_len_igbo, padding='pre', value=0.0\n",
    ")\n",
    "val_target_igbo = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    val_padded_igbo[:, 1:], maxlen=max_len_igbo, padding='pre', value=0.0\n",
    ")\n",
    "test_target_igbo = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    test_padded_igbo[:, 1:], maxlen=max_len_igbo, padding='pre', value=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "125/125 [==============================] - 203s 2s/step - loss: 6.1506 - accuracy: 0.1246 - val_loss: 4.9845 - val_accuracy: 0.1852\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 196s 2s/step - loss: 4.2481 - accuracy: 0.3792 - val_loss: 3.0194 - val_accuracy: 0.5795\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 194s 2s/step - loss: 2.6159 - accuracy: 0.6568 - val_loss: 1.8482 - val_accuracy: 0.7718\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 193s 2s/step - loss: 1.7602 - accuracy: 0.7881 - val_loss: 1.2524 - val_accuracy: 0.8532\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 194s 2s/step - loss: 1.3050 - accuracy: 0.8476 - val_loss: 0.9326 - val_accuracy: 0.8910\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 194s 2s/step - loss: 1.0358 - accuracy: 0.8775 - val_loss: 0.7371 - val_accuracy: 0.9143\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 194s 2s/step - loss: 0.8461 - accuracy: 0.8985 - val_loss: 0.5998 - val_accuracy: 0.9310\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 194s 2s/step - loss: 0.7035 - accuracy: 0.9138 - val_loss: 0.4962 - val_accuracy: 0.9442\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 194s 2s/step - loss: 0.5831 - accuracy: 0.9262 - val_loss: 0.4173 - val_accuracy: 0.9529\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 194s 2s/step - loss: 0.4806 - accuracy: 0.9370 - val_loss: 0.3573 - val_accuracy: 0.9595\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 194s 2s/step - loss: 0.3903 - accuracy: 0.9476 - val_loss: 0.3093 - val_accuracy: 0.9650\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 193s 2s/step - loss: 0.3095 - accuracy: 0.9581 - val_loss: 0.2718 - val_accuracy: 0.9700\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 194s 2s/step - loss: 0.2369 - accuracy: 0.9702 - val_loss: 0.2423 - val_accuracy: 0.9734\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 194s 2s/step - loss: 0.1737 - accuracy: 0.9817 - val_loss: 0.2195 - val_accuracy: 0.9762\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 194s 2s/step - loss: 0.1200 - accuracy: 0.9905 - val_loss: 0.2019 - val_accuracy: 0.9778\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 194s 2s/step - loss: 0.0780 - accuracy: 0.9962 - val_loss: 0.1867 - val_accuracy: 0.9788\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 193s 2s/step - loss: 0.0476 - accuracy: 0.9990 - val_loss: 0.1741 - val_accuracy: 0.9794\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 193s 2s/step - loss: 0.0285 - accuracy: 0.9998 - val_loss: 0.1659 - val_accuracy: 0.9799\n",
      "Epoch 19/20\n",
      "125/125 [==============================] - 194s 2s/step - loss: 0.0180 - accuracy: 1.0000 - val_loss: 0.1581 - val_accuracy: 0.9806\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 194s 2s/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.1523 - val_accuracy: 0.9810\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(\n",
    "    [train_padded_eng, train_padded_igbo], train_target_igbo,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_data=([val_padded_eng, val_padded_igbo], val_target_igbo)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmvgLC6UgJcN"
   },
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "-Te5-8kfgL8s",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 38s 366ms/step - loss: 0.4220 - accuracy: 0.9471\n",
      "Test Loss: 0.42200347781181335\n",
      "Test Accuracy: 0.9471291899681091\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(\n",
    "    [test_padded_eng, test_padded_igbo],\n",
    "    test_target_igbo\n",
    ")\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 36s 352ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate predictions on the test set\n",
    "test_target_igbo = test_padded_igbo[:, 1:]  # Remove start token from targets\n",
    "test_predictions = model.predict([test_padded_eng, test_padded_igbo])\n",
    "\n",
    "# convert predictions to sequences\n",
    "test_predictions_sequences = tokenizer_igbo.sequences_to_texts(\n",
    "    np.argmax(test_predictions, axis=-1)\n",
    ")\n",
    "\n",
    "# convert ground truth to sequences\n",
    "test_target_sequences = tokenizer_igbo.sequences_to_texts(test_target_igbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
